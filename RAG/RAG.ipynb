{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e028e9d",
   "metadata": {},
   "source": [
    "### RAG\n",
    "* RAG allows us to integrate external data sources into LLMs\n",
    "* Introducing external data sources allows us to overcome a key limitation of LLMs, which is that they are only limited to answering questions from their training data only\n",
    "> embed user query --> Vector database -->Retrive relevant docs -->incorporate into model prompt\n",
    "\n",
    "> Load documets --> split documents --> embed --> Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d91129ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "env_path = find_dotenv(filename=\"password.env\", usecwd=True)\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9625b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573094b",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4034ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'rag-paper.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('rag-paper.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "#print(docs[0].page_content)\n",
    "print(docs[0].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121a61c",
   "metadata": {},
   "source": [
    "#### Split\n",
    "> We will need to split the documents into chunks. Larger is not necessarily better. Larger chunks means that retreival can be very slow. **Chunk size** controls for this. If you are coming from the traditional ML world you can think of this as a hyperparameter. **Chunk overlap** parameter ensures that not too much context is loat around the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2579a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 158\n",
      "The paper discusses a method for generating Wikipedia entries by summarizing long sequences of text. It was presented at the Meeting of the Association for Computational Linguistics in July 2019 in Florence, Italy. The authors of the paper include Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. The work is part of the proceedings of the conference and is accessible through the provided DOI and URL links. The paper emphasizes the application of advanced techniques in natural language processing to create concise summaries that can effectively represent longer texts.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Step 1 load the documents\n",
    "loader = PyPDFLoader('rag-paper.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "#Step 2: Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                               chunk_overlap=50,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "#Step3: Create the embeddings and store in vectordb\n",
    "embedding_model = OpenAIEmbeddings(model = \"text-embedding-3-small\",\n",
    "                                   api_key = key,\n",
    "                                   )\n",
    "vectordb = Chroma.from_documents(documents=docs,\n",
    "                                 embedding=embedding_model,\n",
    "                                 collection_name=\"GEN_AI\"\n",
    "                                 )\n",
    "\n",
    "retreiver = vectordb.as_retriever(search_type=\"similarity\",\n",
    "                                   search_kwargs={\"k\":3}\n",
    "                                   )\n",
    "#Step4: Create an LCEL rag chain to retreive\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\",\n",
    "                 api_key=key,\n",
    "                 temperature=0,\n",
    "                 max_completion_tokens=200 \n",
    "                 )\n",
    "\n",
    "rag_chain = ({'context':retreiver,'question':RunnablePassthrough()}| prompt | llm |StrOutputParser())\n",
    "#Step5: Test the rag chain\n",
    "result = rag_chain.invoke(\"Give me a summary of the paper?\")\n",
    "print(result) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
