{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ee4113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4a9836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = find_dotenv(filename=\"password.env\", usecwd=True)\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10e9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f322c600",
   "metadata": {},
   "source": [
    "### Simple workflow to prompt llm.\n",
    ">This uses the llm.invoke method which is more general than client.chat,completions.create that we use with open ai. The open ai method is specific to open ai, while the llm.invoke can work with all model providers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2032bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                 temperature = 0,\n",
    "                 openai_api_key=key\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc4e9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- **Problem-Solving Skills**: Math enhances critical thinking and analytical skills, enabling individuals to approach and solve complex problems in various fields, from science and engineering to finance and everyday life.\\n\\n- **Real-World Applications**: Math is essential in numerous practical applications, such as budgeting, cooking, construction, and technology, helping people make informed decisions and optimize resources.\\n\\n- **Foundation for Advanced Learning**: A strong understanding of math is crucial for pursuing advanced studies in disciplines like physics, computer science, economics, and medicine, as it provides the necessary tools for understanding and modeling real-world phenomena.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 19, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CeMjKIvjyOh2NL4K3Q1aEDUKk6pxn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e3b8fe70-f800-4907-8a8a-609c39a30703-0', usage_metadata={'input_tokens': 19, 'output_tokens': 121, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Why is math useful? Give me 3 bullet points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ce55f",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "* Typically used for \n",
    "    * Examples (few shot prompts)\n",
    "    * Adittional context\n",
    "    * Structured questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92c9ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Explain the concept of data science in simple terms using 3 bullet points'\n"
     ]
    }
   ],
   "source": [
    "# creating a template to explain concepts\n",
    "from langchain_core.prompts import PromptTemplate\n",
    " \n",
    "template = \"Explain the concept of {concept} in simple terms using 3 bullet points\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt = prompt_template.invoke({\"concept\": \"data science\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fb7d5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='- **Learning from Data**: Machine learning is a way for computers to learn patterns and make decisions based on data, rather than being explicitly programmed for every task.\\n\\n- **Improvement Over Time**: As more data is fed into the system, the machine learning model can improve its accuracy and performance, adapting to new information and changing conditions.\\n\\n- **Applications Everywhere**: Machine learning is used in various everyday applications, such as recommending movies, recognizing speech, and detecting spam emails, making technology smarter and more user-friendly.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 21, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CeN0vb2o9jchT3zrfRfjaKNkXbxAh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--b2224cda-e7b6-47fc-baa7-bbbb1a21975d-0' usage_metadata={'input_tokens': 21, 'output_tokens': 105, 'total_tokens': 126, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# full workflow with LLM and prompt template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Write the template string and create a PromptTemplate object\n",
    "template = \"Explain the following concept concisely in 3 bullet points: {concept}\"\n",
    "template = PromptTemplate.from_template(template)\n",
    "\n",
    "#create an appropriate llm\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                 temperature = 0,\n",
    "                 openai_api_key=key\n",
    "                 )\n",
    "# Define the llm chain\n",
    "llm_chain = (prompt_template) | (llm)\n",
    "concept = \"machine learning\"\n",
    "response = llm_chain.invoke({\"concept\": concept})\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735ec58",
   "metadata": {},
   "source": [
    "### ChatPrompt template\n",
    "> This kind of prompt template allows us to use Chat roles in the template. We can use roles to pass example conversations and system messages along with every prompt that is created with this template. This helps structure reponses in the correct format (using example conversations).\n",
    "\n",
    ">The template is structured as a list of tuples. Each tuple has a role and the associtaed prompt with the role. The last tuple is typically the prompt where the user is asking the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7924365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "dt_def = \"\"\"A decision tree is a supervised learning algorithm that uses a tree-like structure \n",
    "to make predictions or classifications based on a series of questions about a dataset. \n",
    "It starts with a root node, branches out through internal nodes (which represent tests on features), \n",
    "and ends at leaf nodes (which are the final decisions or outcomes). Decision trees can be used for both regression and classification tasks \n",
    "and are known for being easy to interpret, as they mimic a flowchart of human decision-making. \"\"\"\n",
    "\n",
    "# Define the template\n",
    "template = [\n",
    "    (\"system\",\"You are a helpful datascience explainer\"),\n",
    "    (\"human\",\"answer this question: what is a descion tree?\"),\n",
    "    (\"ai\",dt_def),\n",
    "    (\"human\", \"answer this question:{dt_question}\")\n",
    "\n",
    "    ]\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                 temperature = 0,\n",
    "                 openai_api_key=key)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(template)\n",
    "llm_chain = (template) | (llm)\n",
    "dt_question = \"How are decision trees used in real-world applications?\"\n",
    "response = llm_chain.invoke({\"dt_question\": dt_question})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4755325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Decision trees are widely used in various real-world applications due to their simplicity, interpretability, and effectiveness. Here are some common applications:\\n\\n1. **Healthcare**: Decision trees can help in diagnosing diseases by analyzing patient symptoms and medical history. For example, they can be used to predict whether a patient has a certain condition based on various health indicators.\\n\\n2. **Finance**: In credit scoring, decision trees can assess the risk of lending to an individual by evaluating factors such as credit history, income, and debt levels. They help in making decisions about loan approvals.\\n\\n3. **Marketing**: Businesses use decision trees to segment customers based on purchasing behavior, demographics, and preferences. This helps in targeting marketing campaigns more effectively.\\n\\n4. **Fraud Detection**: Financial institutions employ decision trees to identify potentially fraudulent transactions by analyzing patterns and anomalies in transaction data.\\n\\n5. **Manufacturing**: Decision trees can optimize production processes by analyzing factors that affect product quality and yield, helping to identify the best conditions for manufacturing.\\n\\n6. **Customer Support**: Automated customer service systems use decision trees to guide users through troubleshooting processes based on their responses to specific questions.\\n\\n7. **Risk Management**: Organizations use decision trees to evaluate risks associated with various projects or investments, helping them make informed decisions.\\n\\n8. **Sports Analytics**: Decision trees can analyze player performance and game strategies, assisting coaches and teams in making tactical decisions.\\n\\nOverall, decision trees are versatile tools that can be applied in numerous fields to facilitate decision-making based on data-driven insights.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 310, 'prompt_tokens': 151, 'total_tokens': 461, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CePuZIedt0ds9PMBkSBBnu4ZSev4G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--54023ce8-15f1-4558-9bca-8d3d5a9909bb-0' usage_metadata={'input_tokens': 151, 'output_tokens': 310, 'total_tokens': 461, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84341729",
   "metadata": {},
   "source": [
    "### Few shot prompt template\n",
    "> The *ChatPromptTemplate* class can handle few examples. If you need to provide more examples then a few shot prompt template is better. You can set up your examples as a pandas dataframe and convert to a dictionary using *pd.to_dict(orient='records)* \n",
    "\n",
    "> Need to provide 2 things\n",
    "* A list of examples\n",
    "* a prompt template that will show how the prompt will be structues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed3b1da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment: 5'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate,FewShotPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Define the examples\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "examples = df.to_dict(orient='records')\n",
    "\n",
    "example_template = PromptTemplate.from_template(\"Review: {review},Sentiment: {rating}\\n\")\n",
    "\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_template,\n",
    "    suffix = \"Review:{input}\",\n",
    "    input_variables = [\"input\"]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                    temperature = 0,\n",
    "                    openai_api_key=key)\n",
    "\n",
    "llm_chain = (prompt_template) | (llm)\n",
    "\n",
    "response = llm_chain.invoke({\"input\": \"This movie is the greatest ever\"})\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0ae8a",
   "metadata": {},
   "source": [
    "### Sequential chains\n",
    "* Some things can only be done sequentially.\n",
    "* Consider a chatbot that plans an itenary\n",
    "    * Recieves destination\n",
    "    * compiles a list of sites\n",
    "    * Chooses top 3 sites and creates an itenary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cd198a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Absolutely! Here are some activities you can enjoy while visiting the must-see attractions in Paris:\\n\\n### Activities Around the Eiffel Tower:\\n1. **Picnic at Champ de Mars**: Grab some fresh baguettes, cheese, and wine from a local market and enjoy a picnic on the lawns of Champ de Mars with a stunning view of the Eiffel Tower.\\n2. **Seine River Cruise**: Take a scenic boat cruise along the Seine River, especially at sunset, to see the Eiffel Tower and other landmarks illuminated at night.\\n3. **Dinner at a Nearby Restaurant**: Enjoy a meal at one of the many restaurants with views of the Eiffel Tower, such as Le Café de l'Homme or Les Ombres.\\n\\n### Activities at the Louvre Museum:\\n1. **Guided Tour**: Consider joining a guided tour to learn more about the history and significance of the artworks, including the Mona Lisa and the Winged Victory of Samothrace.\\n2. **Explore the Tuileries Garden**: After your visit to the Louvre, take a leisurely stroll through the beautiful Tuileries Garden, which is just outside the museum.\\n3. **Attend a Special Exhibition**: Check the Louvre's schedule for any temporary exhibitions that might be taking place during your visit.\\n\\n### Activities Near Notre-Dame Cathedral:\\n1. **Visit Sainte-Chapelle**: Just a short walk from Notre-Dame, this stunning chapel is famous for its breathtaking stained glass windows. It's a must-see for anyone interested in Gothic architecture.\\n2. **Explore Île de la Cité**: Wander around the charming streets of Île de la Cité, where you can find quaint shops, cafes, and the beautiful Place Dauphine.\\n3. **Attend a Concert or Service**: Check if there are any concerts or services happening at Notre-Dame or nearby churches, as they often host beautiful choral performances.\\n\\n### Additional Activities in Paris:\\n- **Montmartre Exploration**: Visit the artistic neighborhood of Montmartre, where you can see the Sacré-Cœur Basilica and explore the charming streets that inspired famous artists.\\n- **Shopping in Le Marais**: Spend some time in the trendy Le Marais district, known for its boutiques, vintage shops, and vibrant atmosphere.\\n- **Culinary Experience**: Take a cooking class or a food tour to learn about French cuisine and taste local delicacies, such as macarons, pastries, and cheeses.\\n\\n### Evening Activities:\\n- **Moulin Rouge Show**: Experience a classic Parisian cabaret show at the Moulin Rouge for a night of entertainment and glamour.\\n- **Night Walk Along the Seine**: Enjoy a romantic evening stroll along the Seine River, taking in the illuminated bridges and landmarks.\\n\\nWith these activities, you'll be able to immerse yourself in the beauty and culture of Paris while enjoying its iconic attractions!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 574, 'prompt_tokens': 262, 'total_tokens': 836, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CeRZt7aH7Wum7N61NUBCWrPgdivBU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--2ccf4472-e3b4-47c5-bee3-271c1e0ae200-0' usage_metadata={'input_tokens': 262, 'output_tokens': 574, 'total_tokens': 836, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "#create destination prompt template\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "destination_prompt = PromptTemplate(input_variables=[\"destination\"],\n",
    "                                    template = \"I am planning a trip to {destination}. Give me a brief overview of the place and suggest 3 must-visit attractions.\")\n",
    "\n",
    "attractions_prompt = PromptTemplate(input_variables=[\"attractions\"],\n",
    "                                   template = \"Based on the following attractions: {attractions}, suggest some activities to do.\")\n",
    "\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                    temperature = 0,\n",
    "                    openai_api_key=key)\n",
    "\n",
    "seq_chain ={'attractions': destination_prompt | llm | StrOutputParser()} | attractions_prompt | llm\n",
    "response = seq_chain.invoke({\"destination\": \"Paris\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f610fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "skill_gap_template = PromptTemplate(input_variables=[\"current_skills\"],\n",
    "                                    template=\"\"\"Given the skills required for a good data scientist \n",
    "                                    and the current skills {current_skills}, identify the skill gaps. \\\n",
    "                                    \"Keep your response brief and to the point\"\"\"\n",
    "                                    )\n",
    "\n",
    "project_recco_template = PromptTemplate(input_variables=[\"skill_gaps\"],\n",
    "                                    template=\"\"\"Based on the following skill gaps: {skill_gaps}, \n",
    "                                    suggest 2 project ideas that can help in acquiring these skills. \n",
    "                                    \"Provide a brief description for each project idea.\"\"\"\n",
    "                                    )\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                    temperature = 0,\n",
    "                    openai_api_key=key)\n",
    "\n",
    "seq_chain = {'skill_gaps': skill_gap_template | llm | StrOutputParser()} | project_recco_template | llm\n",
    "response = seq_chain.invoke({\"current_skills\": \"Python, SQL, Basic Statistics\"})\n",
    "\n",
    "with open(\"project_ideas.txt\",\"w\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341bfa4",
   "metadata": {},
   "source": [
    "### React agents\n",
    "* Agents use LLMs to take actions\n",
    "* Agents can have functions or tools to interact with the environment\n",
    "* React stands for reasoning and action. Observe --> think --> act\n",
    "* Use langgraph to design agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e21773ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_29416\\1218472377.py:11: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm,tools=tool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of 256 plus 100 is 116.0.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                    temperature = 0,\n",
    "                    openai_api_key=key)\n",
    "\n",
    "tool = load_tools([\"llm-math\"],\n",
    "                  llm = llm)\n",
    "\n",
    "agent = create_react_agent(llm,tools=tool)\n",
    "\n",
    "fnl_message = agent.invoke({'messages':[('system','When the user asks any question involving '\n",
    "'math or arithmetic, always convert the question into a clear mathematical expression and '\n",
    "'call the calculator tool. Do not answer math questions without using the tool.')\n",
    "    ,('human',\"What is the square root of 256 plus 100?\")]})\n",
    "print(fnl_message['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daad7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_29416\\3443211687.py:7: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current population of Nepal is approximately 29,164,578, according to the 2021 census. The population growth rate is about 0.92% per year.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n",
    "tool = load_tools([\"wikipedia\"])\n",
    "\n",
    "agent = create_react_agent(llm,\n",
    "                           tools=tool)\n",
    "\n",
    "response=agent.invoke({'messages':[('human','what is the population of Nepal?')]})\n",
    "\n",
    "response['messages'][-1].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153894d0",
   "metadata": {},
   "source": [
    "#### Design custom tools\n",
    "> Till now we have only loaded already available tools. The real power is when you can write your own custom tools for agents. Tools are generally built from Python functions and use a tool decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_29416\\567845599.py:18: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(llm,tools=[create_financial_report])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the financial report for ABC Corp:\n",
      "\n",
      "- **Total Revenue:** $1,000,000.00\n",
      "- **Total Expenses:** $750,000.00\n",
      "- **Net Income:** $250,000.00\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "@tool\n",
    "def create_financial_report(revenue:float,expenses:float,company_name:str):\n",
    "    \"\"\" Generates a simple income statemrnt for a comapny given revenue and expenses\"\"\"\n",
    "    net_income = revenue - expenses\n",
    "    report = f\"Financial report for {company_name}:\\n\"\n",
    "    report += f\"Total revenue: ${revenue}\\n\"\n",
    "    report += f\"Total expenses: ${expenses}\\n\"\n",
    "    report += f\"Net income: ${net_income}\\n\"\n",
    "    return report\n",
    "\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                 temperature = 0,\n",
    "                 openai_api_key=key)\n",
    "\n",
    "agent = create_react_agent(llm,tools=[create_financial_report])\n",
    "response=agent.invoke({'messages':[('human','Create a financial report for ABC Corp with revenue of 1,000,000 and expenses of 750,000')]})  \n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fde62e",
   "metadata": {},
   "source": [
    "## RAG (Retrieval Augmented generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7671df",
   "metadata": {},
   "source": [
    "### Document loaders\n",
    "> Pre-trained LLMs do not have access to external data. Their understanding comes purely from their training data. RAG uses embeddings to retrieve the most relevant documents from the database in answer to the users queries which are themselves converted into embeddings\n",
    "* Load the documents using document loaders\n",
    "* Split the documents into chunks\n",
    "* Encode and store the chunks for retreival\n",
    "\n",
    "> Langchain provides document loader classes for common file types such as pdfs and csvs\n",
    "\n",
    "> After loading documents we split them into chunks. Chunking is particularly useful because chunks can be fit into an LLMs context window\n",
    "\n",
    "##### Chunking startegies\n",
    "* You can chunk by line. This is very simple but sentences can be split into multiple lines. Creating a chunk overlap can retain context between chunks. If the model performance on queries related to the external data source is poor then you might need to increase the chunk overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de13d871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Reviews.csv', 'row': 0}, page_content='review: Great movie\\nrating: 5')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "loader = CSVLoader(file_path='Reviews.csv',encoding='utf-8')\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0340c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33, 38, 14, 42, 68, 86, 1, 88, 1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = \"\"\" Life is like a box of chocolates. You never know what you're gonna get. This is what\n",
    "Forest Gump said in the movie Forest Gump. The quote means that life is full of surprises and uncertainties, \n",
    "just like a box of chocolates where you can't see what's inside until you take a bite. \n",
    "It encourages us to embrace the unpredictability of life and be open to new experiences.\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 5,\n",
    "    separators= ['.','\\n']\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(quote)\n",
    "[len(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dcaac",
   "metadata": {},
   "source": [
    "#### Storage and retreival using Vector Datbases\n",
    "* the workflow\n",
    "    > Load Document --> Split Text --> Storage and retreival for user queries\n",
    "* We use Vector databases to store and retreive information\n",
    "    > User prompt --> embedding model --> Vector Database --> Retreive similar documents --> Insert similar docs into prompt template --> feeed the response to the LLM\n",
    "* For choosing Vector databases the key considerations are\n",
    "    * Open source vs a closed ecosystem. Open source is much more flexible but might have security challenges\n",
    "    * Cloud vs an on-prem solution\n",
    "    * Lightweight vs powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d657fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "# Creating an object of the OpenAIEmbeddings class with model text-embedding-small\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                              openai_api_key=key)\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path='Reviews.csv',encoding='utf-8')\n",
    "data = loader.load()\n",
    "\n",
    "#split the documents into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 20,\n",
    "    chunk_overlap = 10,\n",
    "    separators= ['.','\\n']\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# Create the Chroma vector store\n",
    "vector_store = Chroma.from_documents(documents=docs,\n",
    "                                        embedding=embedding_function,\n",
    "                                        collection_name=\"reviews_collection\",\n",
    "                                        persist_directory=\"./chroma_db\"\n",
    "                                        )\n",
    "                                     \n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e4fabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "message = \"\"\" Given the folloawing reviews as examples\n",
    "             {reviews}\n",
    "             Give me the rating of the review: {input}\n",
    "             rating:\n",
    "             \"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([('human',message)])\n",
    "\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\",\n",
    "                    temperature = 0,\n",
    "                    openai_api_key=key)\n",
    "\n",
    "ragchain = ({'reviews':retriever,'input':RunnablePassthrough()} | template | llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce9c1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the review \"This was a movie with good acting and a good storyline,\" it seems to convey a positive sentiment. While the exact rating isn't provided, a reasonable assumption could be a rating of 4 or 5 out of 5, given the praise for both acting and storyline. \n",
      "\n",
      "If you need a specific rating, please provide a scale or criteria for rating.\n"
     ]
    }
   ],
   "source": [
    "response = ragchain.invoke(\"This was a movie with good acting and a good storyline\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a062d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
