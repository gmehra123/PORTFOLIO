{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmehra123/PORTFOLIO/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6btafu5ScvjX"
      },
      "source": [
        "## Lang chain\n",
        "#### Lang chain intro\n",
        "* Lang chain is a framework for integrating AI models into workflows\n",
        "* **Langchain** is part of a larger system that contains **Langgraph** for AI agents and **Langsmith** for deploying apps\n",
        "* There are many elements to the langchain workflow.\n",
        "  * LLM model\n",
        "  * Decisions to be made\n",
        "  * Database\n",
        "  * Retreival\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gaura\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_huggingface import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.exists(r\"C:\\Users\\gaura\\PORTFOLIO\\password.env\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gaura\\PORTFOLIO\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded environment from: c:\\Users\\gaura\\PORTFOLIO\\password.env\n",
            "OPENAI_API_KEY: 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\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "\n",
        "env_path = find_dotenv(filename=\"password.env\", usecwd=True)\n",
        "if env_path:\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"✅ Loaded environment from: {env_path}\")\n",
        "else:\n",
        "    print(\"⚠️ No .env file found\")\n",
        "\n",
        "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define llm step\n",
        "llm = ChatOpenAI(model='gpt-4o-mini',\n",
        "                 api_key = key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is a framework designed for building applications using language models. It provides tools and components to facilitate tasks such as connecting language models with various data sources, managing conversational state, and integrating with APIs. LangChain aims to simplify the development of applications that leverage natural language processing capabilities, enabling developers to create more complex and interactive AI-driven experiences.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 14, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZ9mUGMncwpvNRaDLO8SanoN2fYui', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7662b516-940e-4270-82ec-1e30bd6a24ef-0', usage_metadata={'input_tokens': 14, 'output_tokens': 68, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What is langchain? Be brief\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # We can also use Hugging face\n",
        "# from transformers import pipeline\n",
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# # ✅ this uses AutoModelForCausalLM, the correct class for GPT-style models\n",
        "# generator = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=\"distilgpt2\",          # or EleutherAI/gpt-neo-125M, facebook/opt-125m, etc.\n",
        "#     device_map=\"auto\",           # CPU or GPU automatically\n",
        "#     torch_dtype=\"auto\"\n",
        "# )\n",
        "\n",
        "# # Wrap for LangChain compatibility\n",
        "# llm = HuggingFacePipeline(pipeline=generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt templates\n",
        "* Contains instructions examples and any additional context\n",
        "* Created using *langchain_core.prompts*  import *PromptTemplate* class\n",
        "* We can create a template from the *from_template* method of *PromptTemplate*\n",
        "* We can then use pipe symbol to connect the prompt to an llm setting up a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# llm = HuggingFacePipeline.from_model_id(\n",
        "# model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "# task=\"text-generation\",\n",
        "# pipeline_kwargs={\"max_new_tokens\": 100}\n",
        "# )\n",
        "# llm.invoke(\"What is Hugging Face?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"The Taylor Series is a mathematical representation of a function as an infinite sum of terms calculated from the values of its derivatives at a single point. It allows us to approximate a function near that point using polynomials. \\n\\nThe Taylor series of a function \\\\( f(x) \\\\) around the point \\\\( a \\\\) is given by:\\n\\n\\\\[\\nf(x) = f(a) + f'(a)(x - a) + \\\\frac{f''(a)}{2!}(x - a)^2 + \\\\frac{f'''(a)}{3!}(x - a)^3 + \\\\ldots\\n\\\\]\\n\\nIn summation notation, this can be expressed as:\\n\\n\\\\[\\nf(x) = \\\\sum_{n=0}^{\\\\infty} \\\\frac{f^{(n)}(a)}{n!}(x - a)^n\\n\\\\]\\n\\nwhere \\\\( f^{(n)}(a) \\\\) is the \\\\( n \\\\)-th derivative of \\\\( f \\\\) evaluated at \\\\( a \\\\), and \\\\( n! \\\\) is the factorial of \\\\( n \\\\). The Taylor series provides a powerful tool for approximating functions and analyzing their behavior near the point \\\\( a \\\\).\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 17, 'total_tokens': 262, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZQWBvTuNdG0hW1kjviLPcr93QRcB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--eb9831a6-5b6a-4550-984d-93aeaa3c9b96-0', usage_metadata={'input_tokens': 17, 'output_tokens': 245, 'total_tokens': 262, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Single Prompt Template\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "template = \"Explain the following concept briefly:{concept}\"\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "prompt = prompt_template.invoke({'concept':'Explain the Taylor series'})\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini',\n",
        "                 api_key = key)\n",
        "llm_chain = prompt_template | llm\n",
        "llm_chain.invoke({'concept':'Explain the Taylor Series'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'32 + 32 equals 64.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ChatPromptTemplate used for chat bots to guide conversation flow\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template =[\n",
        "    ('system','You are a helpful math assistant that answers calculation problems with math'),\n",
        "    ('human','answer this math question: What is 5X2'),\n",
        "    ('ai','5X2 is 10'),\n",
        "    ('human','answer this math question {math}')\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(template)\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini',\n",
        "                 api_key=key)\n",
        "llm_chain = prompt_template | llm\n",
        "response = llm_chain.invoke({'math':'What is 32+32'})\n",
        "response.content"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMl9jywJor7OkA7wxvL04iC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
