{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmehra123/PORTFOLIO/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6btafu5ScvjX"
      },
      "source": [
        "## Lang chain\n",
        "#### Lang chain intro\n",
        "* Lang chain is a framework for integrating AI models into workflows\n",
        "* **Langchain** is part of a larger system that contains **Langgraph** for AI agents and **Langsmith** for deploying apps\n",
        "* There are many elements to the langchain workflow.\n",
        "  * LLM model\n",
        "  * Decisions to be made\n",
        "  * Database\n",
        "  * Retreival\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from langchain_huggingface import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.exists(r\"C:\\Users\\gaura\\PORTFOLIO\\password.env\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gaura\\PORTFOLIO\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded environment from: c:\\Users\\gaura\\PORTFOLIO\\password.env\n",
            "OPENAI_API_KEY: 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\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "\n",
        "env_path = find_dotenv(filename=\"password.env\", usecwd=True)\n",
        "if env_path:\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"✅ Loaded environment from: {env_path}\")\n",
        "else:\n",
        "    print(\"⚠️ No .env file found\")\n",
        "\n",
        "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define llm step\n",
        "llm = ChatOpenAI(model='gpt-4o-mini',\n",
        "                 api_key = key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is a framework designed for building applications using language models. It provides tools and components to facilitate tasks such as connecting language models with various data sources, managing conversational state, and integrating with APIs. LangChain aims to simplify the development of applications that leverage natural language processing capabilities, enabling developers to create more complex and interactive AI-driven experiences.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 14, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZ9mUGMncwpvNRaDLO8SanoN2fYui', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7662b516-940e-4270-82ec-1e30bd6a24ef-0', usage_metadata={'input_tokens': 14, 'output_tokens': 68, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What is langchain? Be brief\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # We can also use Hugging face\n",
        "# from transformers import pipeline\n",
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# # ✅ this uses AutoModelForCausalLM, the correct class for GPT-style models\n",
        "# generator = pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=\"distilgpt2\",          # or EleutherAI/gpt-neo-125M, facebook/opt-125m, etc.\n",
        "#     device_map=\"auto\",           # CPU or GPU automatically\n",
        "#     torch_dtype=\"auto\"\n",
        "# )\n",
        "\n",
        "# # Wrap for LangChain compatibility\n",
        "# llm = HuggingFacePipeline(pipeline=generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt templates\n",
        "* Contains instructions examples and any additional context\n",
        "* Created using *langchain_core.prompts*  import *PromptTemplate* class\n",
        "* We can create a template from the *from_template* method of *PromptTemplate*\n",
        "* We can then use pipe symbol to connect the prompt to an llm setting up a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# llm = HuggingFacePipeline.from_model_id(\n",
        "# model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "# task=\"text-generation\",\n",
        "# pipeline_kwargs={\"max_new_tokens\": 100}\n",
        "# )\n",
        "# llm.invoke(\"What is Hugging Face?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"The Taylor Series is a mathematical representation of a function as an infinite sum of terms calculated from the values of its derivatives at a single point. It allows us to approximate a function near that point using polynomials. \\n\\nThe Taylor series of a function \\\\( f(x) \\\\) around the point \\\\( a \\\\) is given by:\\n\\n\\\\[\\nf(x) = f(a) + f'(a)(x - a) + \\\\frac{f''(a)}{2!}(x - a)^2 + \\\\frac{f'''(a)}{3!}(x - a)^3 + \\\\ldots\\n\\\\]\\n\\nIn summation notation, this can be expressed as:\\n\\n\\\\[\\nf(x) = \\\\sum_{n=0}^{\\\\infty} \\\\frac{f^{(n)}(a)}{n!}(x - a)^n\\n\\\\]\\n\\nwhere \\\\( f^{(n)}(a) \\\\) is the \\\\( n \\\\)-th derivative of \\\\( f \\\\) evaluated at \\\\( a \\\\), and \\\\( n! \\\\) is the factorial of \\\\( n \\\\). The Taylor series provides a powerful tool for approximating functions and analyzing their behavior near the point \\\\( a \\\\).\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 17, 'total_tokens': 262, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CZQWBvTuNdG0hW1kjviLPcr93QRcB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--eb9831a6-5b6a-4550-984d-93aeaa3c9b96-0', usage_metadata={'input_tokens': 17, 'output_tokens': 245, 'total_tokens': 262, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Single Prompt Template\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "template = \"Explain the following concept briefly:{concept}\"\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "prompt = prompt_template.invoke({'concept':'Explain the Taylor series'})\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini',\n",
        "                 api_key = key)\n",
        "llm_chain = prompt_template | llm\n",
        "llm_chain.invoke({'concept':'Explain the Taylor Series'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'32 + 32 equals 64.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ChatPromptTemplate used for chat bots to guide conversation flow\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template =[\n",
        "    ('system','You are a helpful math assistant that answers calculation problems with math'),\n",
        "    ('human','answer this math question: What is 5X2'),\n",
        "    ('ai','5X2 is 10'),\n",
        "    ('human','answer this math question {math}')\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(template)\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini',\n",
        "                 api_key=key)\n",
        "llm_chain = prompt_template | llm\n",
        "response = llm_chain.invoke({'math':'What is 32+32'})\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Revies is:shitty movie\n",
            "rating is 1\n",
            "Revies is:Outstanding movie\n",
            "rating is 5\n",
            "\n",
            "Revies is:Good movie but could be better\n",
            "rating is 3\n",
            "\n",
            "Revies is:Was boring and a waste of time\n",
            "rating is 1\n",
            "\n",
            "Review is This is an ok movie\n"
          ]
        }
      ],
      "source": [
        "#Few shot prompt template\n",
        "from langchain_core.prompts import FewShotPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "examples = [\n",
        "    {'review':'Outstanding movie','rating':5},\n",
        "    {'review':'Good movie but could be better', 'rating':3},\n",
        "    {'review':'Was boring and a waste of time', 'rating':1}\n",
        "\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate.from_template('Revies is:{review}\\nrating is {rating}') \n",
        "\n",
        "prompt = example_prompt.invoke({'review':'shitty movie','rating':1})\n",
        "print(prompt.text)\n",
        "\n",
        "prompt_fnl_template = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    suffix = 'Review is {Input}',\n",
        "    input_variables = [\"Input\"]\n",
        "\n",
        ")\n",
        "\n",
        "prompt_fnl = prompt_fnl_template.invoke({'Input':'This is an ok movie'})\n",
        "print(prompt_fnl.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequential chains\n",
        "* Some problems can only be solved sequentialls\n",
        "* Itenary chatbot\n",
        "    * Tell destination\n",
        "    * Recieve suggestions on what sights to see\n",
        "    * Select the activities that are interesting\n",
        "    * Then compile the itenary\n",
        "* All these tasks happen in a sequence\n",
        "* In sequential chains the output from 1 step becomes the input to the next\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here’s a detailed 3-day itinerary for visiting Rome, highlighting key attractions, cultural experiences, and culinary delights.\n",
            "\n",
            "### Day 1: Ancient Rome\n",
            "**Morning:**\n",
            "- **Colosseum**: Start your day at the Colosseum, Rome’s iconic symbol. It’s best to book a guided tour to skip the lines and learn about its fascinating history.\n",
            "- **Roman Forum**: After exploring the Colosseum, take a short walk to the Roman Forum, the heart of ancient Rome's public life.\n",
            "\n",
            "**Lunch:**\n",
            "- Enjoy a traditional Roman meal at **Trattoria da Teo** in Trastevere, known for its pasta dishes like Carbonara.\n",
            "\n",
            "**Afternoon:**\n",
            "- **Palatine Hill**: Discover the birthplace of Rome and enjoy breathtaking views of the city.\n",
            "- **Capitoline Museums**: If time permits, visit the museums to see Roman sculptures and artworks.\n",
            "\n",
            "**Evening:**\n",
            "- **Dinner**: Try **Piperno** in the Jewish Ghetto for local specialties, such as fried artichokes.\n",
            "- **Explore**: Stroll along the Tiber River after dinner and take in the nightlife.\n",
            "\n",
            "### Day 2: Vatican City\n",
            "**Morning:**\n",
            "- **Vatican Museums**: Get an early start and head to the Vatican Museums. Don’t miss the Sistine Chapel with Michelangelo’s incredible ceiling.\n",
            "  \n",
            "**Lunch:**\n",
            "- Head to **Pizzarium**, a renowned pizza al taglio (by-the-slice) spot near the Vatican.\n",
            "\n",
            "**Afternoon:**\n",
            "- **St. Peter’s Basilica**: Visit this magnificent basilica and consider climbing up to the dome for a stunning panoramic view of the city.\n",
            "  \n",
            "**Evening:**\n",
            "- **Dinner**: Enjoy a meal at **Osteria dell’Anima**, known for its cozy ambiance and Italian classics.\n",
            "- **Explore**: Take a leisurely walk at St. Peter's Square and admire its grandeur at night.\n",
            "\n",
            "### Day 3: Baroque Rome and Hidden Gems\n",
            "**Morning:**\n",
            "- **Piazza Navona**: Start your day at this beautiful square, famous for its Baroque architecture and fountains.\n",
            "- **Pantheon**: Visit this ancient temple and marvel at its impressive dome.\n",
            "\n",
            "**Lunch:**\n",
            "- Grab a bite at **Tazza d’Oro**, famous for its coffee and pastries, or enjoy light bites at nearby cafés.\n",
            "\n",
            "**Afternoon:**\n",
            "- **Trevi Fountain**: Toss a coin in the fountain and make a wish.\n",
            "- **Spanish Steps**: Climb the steps and enjoy the views from the top at the Trinità dei Monti.\n",
            "  \n",
            "**Evening:**\n",
            "- **Dinner**: Dine at **Ristorante Da Francesco** near Piazza Navona for classic Roman cuisine.\n",
            "- **Gelato**: End your trip with gelato at **Giolitti**, one of the oldest gelaterias in Rome.\n",
            "\n",
            "### Tips:\n",
            "- **Transportation**: Use public transportation (buses, metro) or consider walking; many attractions are close together.\n",
            "- **Tickets**: Pre-book tickets for the Colosseum, Vatican Museums, and other popular sites to avoid long lines.\n",
            "- **Dress Code**: When visiting churches, ensure to dress appropriately (shoulders and knees covered).\n",
            "- **Stay Hydrated**: Don’t forget to drink plenty of water and try the numerous public drinking fountains around the city.\n",
            "\n",
            "Enjoy your trip to Rome! The city is a treasure trove of history, culture, and mouthwatering cuisine.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "dest_prompt = PromptTemplate( input_variables=[\"destination\"],\n",
        "                             template = 'I want to go to {destination}')\n",
        "sights_prompt = PromptTemplate(input_variables=[\"sights\"],\n",
        "                               template = 'I have 3 days. ' \\\n",
        "                               'Can you create an itenary to visit the top 3 {sights}')\n",
        "\n",
        "llm = ChatOpenAI(model = 'gpt-4o-mini',\n",
        "                 api_key = key)\n",
        "\n",
        "llm_chain = ({'sights': dest_prompt | llm | StrOutputParser()} \n",
        "             | sights_prompt | llm | StrOutputParser())\n",
        "\n",
        "response = llm_chain.invoke({'destination':'Rome'})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agents introduction\n",
        "* Simplest type of agent is the react agent. React stands for reasoning and action\n",
        "* The agent prompts the model using thinking acting and observing\n",
        "* If we were to ask a react agent what is the weather like in Kingston Jamaica.\n",
        "    * The model will think about the task and which tool to call\n",
        "    * Call the tool return the output and observe it and summarize it.\n",
        "    * The agent will then be ready for the next call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_32072\\1661241875.py:10: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(llm,tools)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': [HumanMessage(content='What is 215+876?', additional_kwargs={}, response_metadata={}, id='b3b0343a-0125-4c70-83f8-c78a424d9a2b'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 61, 'total_tokens': 79, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaF0qdvMvDijK73swtsO4MWyYAyyF', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--262a417c-bf97-4d5d-bb70-b8433f6c8580-0', tool_calls=[{'name': 'Calculator', 'args': {'__arg1': '215+876'}, 'id': 'call_b0nygkUi0fKoWg8pL1UN6fSr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 18, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Answer: 1091', name='Calculator', id='9ab369a3-d9a8-4439-bbeb-d99242ec3e75', tool_call_id='call_b0nygkUi0fKoWg8pL1UN6fSr'), AIMessage(content='The sum of 215 and 876 is 1091.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 91, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaF0tG2Q0uxBltq7F9bfYGflddlBW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--762a2626-e2ca-450f-ab17-75c7b93404f9-0', usage_metadata={'input_tokens': 91, 'output_tokens': 14, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
          ]
        }
      ],
      "source": [
        "# Now looking at agents\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4o-mini',\n",
        "                 api_key=key)\n",
        "tools = load_tools([\"llm-math\"],\n",
        "                   llm = llm)\n",
        "\n",
        "agent = create_react_agent(llm,tools)\n",
        "\n",
        "messages = agent.invoke({'messages':[('human','What is 215+876?')]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The sum of 215 and 876 is 1091.\n"
          ]
        }
      ],
      "source": [
        "print(messages['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Tools\n",
        "* Tools must be formatted in a certain way for langchain\n",
        "* They must have a name accesible via the name attribute\n",
        "* You can use python functions with the @tool decorator to create tools\n",
        "* Name--> Is automatically set to the function name\n",
        "* Description --> Is automatically set to the function doctring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculator\n",
            "Useful for when you need to answer questions about math.\n"
          ]
        }
      ],
      "source": [
        "tools = load_tools([\"llm-math\"],\n",
        "                   llm = llm)\n",
        "#Get tool name\n",
        "print(tools[0].name)\n",
        "\n",
        "# Get the tool description\n",
        "print(tools[0].description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMl9jywJor7OkA7wxvL04iC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
